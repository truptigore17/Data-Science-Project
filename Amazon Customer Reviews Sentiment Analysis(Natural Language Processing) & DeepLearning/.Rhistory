#install.packages('plyr')
#install.packages("stringr")
library(tm)
library(RTextTools)
library(rvest)
library(dplyr)
library(plyr)
library(stringr)
options(max.print=1000000)
#count for number of pages.
pages <- 750
all_reviews <- NULL
for(page_num in 1:pages){
#reading one page at a time
single_page <- paste0("https://www.amazon.com/All-New-Fire-TV-Stick-With-Alexa-Voice-Remote-Streaming-Media-Player/product-reviews/B00ZV9RDKK/ref=cm_cr_getr_d_show_all?ie=UTF8&reviewerType=all_reviews&showViewpoints=1&sortBy=helpful&pageNumber=", page_num)
#Converting to HTML
single_doc <- read_html(single_page)
#Parsing review text
review <-html_nodes(x= single_doc, css = ".review-text") %>%
html_text()
#Matrix conversion of data for columner form
review <- as.matrix(review)
#consolidating all reviews in one column
all_reviews <- rbind(all_reviews,review)
}
View(all_reviews)
iniReviews <- all_reviews
all_reviews <- iniReviews
#Initial cleaning of data
#Removing '/' character
all_reviews <- gsub("/", "", all_reviews)
#Removing '\' character
all_reviews <- gsub("'\'", "", all_reviews)
#Removing line breaks
all_reviews <- gsub("[\r\n]", " ", all_reviews)
#Removing garbage
all_reviews <- gsub("u0096", "", all_reviews)
#Removing Punctuation Marks
all_reviews <- gsub("[[:punct:]]", "", all_reviews)
#Removing digits or numbers.
all_reviews <- gsub('\\d+', '', all_reviews)
#Remove Unassigned & Other symbols like emojis
all_reviews <-gsub('\\p{So}|\\p{Cn}', '', all_reviews, perl = TRUE)
#Creating Corpus
myCorpus <- Corpus(VectorSource(all_reviews))
inspect(myCorpus)
#Cleaning Corpus
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
#Strip-off white spaces,plain text and lowercase
myCorpus <- tm_map(myCorpus,stripWhitespace)
myCorpus <- tm_map(myCorpus,PlainTextDocument)
myCorpus <- tm_map(myCorpus, tolower)
# remove stopwords
myCorpus <- tm_map(myCorpus,removeWords,stopwords("english"))
#Clearing whitespace again
myCorpus <- tm_map(myCorpus,stripWhitespace)
#Tokenization
scan_tokenizer <- function(x)
scan(text = as.character(x), what = "character", quote = "",
quiet = TRUE)
token <- Token_Tokenizer(scan_tokenizer)
#testtokenized_reviews <- token(myCorpus)
#positive word list stemming
poslist <- read.delim(file="E:/ADS/Final Project/positive-words.txt", header=FALSE, stringsAsFactors=FALSE)
View(poslist)
names(poslist) <- c('posword')
poslist <- wordStem(as.matrix(poslist))
#negative word list stemming
neglist <- read.delim(file="E:/ADS/Final Project/negative-words.txt", header=FALSE, stringsAsFactors=FALSE)
View(neglist)
names(neglist) <- c('negword')
neglist <- wordStem(as.matrix(neglist))
#function to calculate review sentiments scores
sentimentAnalysis <- function(all_reviews, neglist, poslist){
result <- matrix('', 0, 3)
reviews_scores <- laply(all_reviews, function(review, neglist, poslist){
#preserving review text
initial_review <- review
#creating wordlist of review
tokenized_review <- token(review)
#stemming review list
stemmed_review <- wordStem(tokenized_review)
#print(stemmed_review)
#Cleaning the stemmed text
stemmed_review <- gsub('[[:punct:]]', '', stemmed_review )
#Removing Control Characters
stemmed_review  <- gsub('[[:cntrl:]]', '', stemmed_review)
#List with matches between review words and each category(positive & negative)
positiveMatch_list <- match(stemmed_review, poslist)
negativeMatch_list <- match(stemmed_review, neglist)
#calculate sum of number of words in each matched category
positiveMatch_list <- sum(!is.na(positiveMatch_list))
negativeMatch_list <- sum(!is.na(negativeMatch_list))
#total <- positiveMatch_list + negativeMatch_list
#posRatio <- (positiveMatch_list/total)
#negRatio <- (negativeMatch_list/total)
score <- c(positiveMatch_list, negativeMatch_list)
#add row to scores table
newrow <- c(initial_review, score)
result  <- rbind(result, newrow)
return(result)
}, neglist, poslist)
return(reviews_scores)
}
#Review table with scores
fin_Result <- as.data.frame(sentimentAnalysis(myCorpus, neglist, poslist))
colnames(fin_Result) <- c('Review', 'posRatio', 'negRatio')
View(fin_Result)
fin_Result[,1] <- as.character( fin_Result[, 1] )
fin_Result[,2] <- as.numeric(as.character( fin_Result[, 2] ))
fin_Result[,3] <- as.numeric(as.character( fin_Result[, 3] ))
##########################################################
#Creating new column with result of positive/negative values
fin_Result["Result"] <-NA
for(i in 1:dim(fin_Result)[1]){
if(fin_Result[i,2]>fin_Result[i,3]) {
fin_Result[i,4]<-1
}else if(fin_Result[i,2]==fin_Result[i,3]) {
fin_Result[i,4]<-0
}else{
fin_Result[i,4]<--1
}
}
# Convert corpus to Stem document
corpus_clean <- tm_map(myCorpus, stemDocument,language="english")
inspect(corpus_clean)
#Transform Corpus to Document Term Matrix
corpus_dtm <- DocumentTermMatrix(corpus_clean)
inspect(corpus_dtm)
#Writing output in csv file
final<-write.csv(cbind(as.matrix(corpus_dtm),fin_Result$Result),"E:/ADS/Final Project/final.csv")
#inspect(corpus_dtm[1:10,1:10])
#  TDM <- TermDocumentMatrix(doc.corpus)
#DBN
final<-write.csv(cbind(as.matrix(corpus_dtm),fin_Result$Result),"E:/ADS/Final Project/final.csv")
fin_Result["Result"] <-NA
for(i in 1:dim(fin_Result)[1]){
if(fin_Result[i,2]>fin_Result[i,3]) {
fin_Result[i,4]<-1
}else if(fin_Result[i,2]==fin_Result[i,3]) {
fin_Result[i,4]<-0
}else{
fin_Result[i,4]<--1
}
}
fin_Result <- as.data.frame(sentimentAnalysis(myCorpus, neglist, poslist))
fin_Result <- as.data.frame(sentimentAnalysis(myCorpus, neglist, poslist))
fin_Result <- as.data.frame(sentimentAnalysis(myCorpus, neglist, poslist))
fin_Result <- as.data.frame(sentimentAnalysis(myCorpus, neglist, poslist))
tokenized_review <- token(review)
stemmed_review <- wordStem(tokenized_review)
library(tm)
library(RTextTools)
library(rvest)
library(dplyr)
library(plyr)
library(stringr)
options(max.print=1000000)
#count for number of pages.
pages <- 750
all_reviews <- NULL
for(page_num in 1:pages){
#reading one page at a time
single_page <- paste0("https://www.amazon.com/All-New-Fire-TV-Stick-With-Alexa-Voice-Remote-Streaming-Media-Player/product-reviews/B00ZV9RDKK/ref=cm_cr_getr_d_show_all?ie=UTF8&reviewerType=all_reviews&showViewpoints=1&sortBy=helpful&pageNumber=", page_num)
#Converting to HTML
single_doc <- read_html(single_page)
#Parsing review text
review <-html_nodes(x= single_doc, css = ".review-text") %>%
html_text()
#Matrix conversion of data for columner form
review <- as.matrix(review)
#consolidating all reviews in one column
all_reviews <- rbind(all_reviews,review)
}
View(all_reviews)
iniReviews <- all_reviews
all_reviews <- iniReviews
#Initial cleaning of data
#Removing '/' character
all_reviews <- gsub("/", "", all_reviews)
#Removing '\' character
all_reviews <- gsub("'\'", "", all_reviews)
#Removing line breaks
all_reviews <- gsub("[\r\n]", " ", all_reviews)
#Removing garbage
all_reviews <- gsub("u0096", "", all_reviews)
#Removing Punctuation Marks
all_reviews <- gsub("[[:punct:]]", "", all_reviews)
#Removing digits or numbers.
all_reviews <- gsub('\\d+', '', all_reviews)
#Remove Unassigned & Other symbols like emojis
all_reviews <-gsub('\\p{So}|\\p{Cn}', '', all_reviews, perl = TRUE)
myCorpus <- Corpus(VectorSource(all_reviews))
inspect(myCorpus)
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus,stripWhitespace)
myCorpus <- tm_map(myCorpus,PlainTextDocument)
myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus,removeWords,stopwords("english"))
myCorpus <- tm_map(myCorpus,stripWhitespace)
scan_tokenizer <- function(x)
scan(text = as.character(x), what = "character", quote = "",
quiet = TRUE)
token <- Token_Tokenizer(scan_tokenizer)
poslist <- read.delim(file="E:/ADS/Final Project/positive-words.txt", header=FALSE, stringsAsFactors=FALSE)
names(poslist) <- c('posword')
poslist <- wordStem(as.matrix(poslist))
neglist <- read.delim(file="E:/ADS/Final Project/negative-words.txt", header=FALSE, stringsAsFactors=FALSE)
names(neglist) <- c('negword')
neglist <- wordStem(as.matrix(neglist))
sentimentAnalysis <- function(all_reviews, neglist, poslist){
result <- matrix('', 0, 3)
reviews_scores <- laply(all_reviews, function(review, neglist, poslist){
#preserving review text
initial_review <- review
#creating wordlist of review
tokenized_review <- token(review)
#stemming review list
stemmed_review <- wordStem(tokenized_review)
#print(stemmed_review)
#Cleaning the stemmed text
stemmed_review <- gsub('[[:punct:]]', '', stemmed_review )
#Removing Control Characters
stemmed_review  <- gsub('[[:cntrl:]]', '', stemmed_review)
#List with matches between review words and each category(positive & negative)
positiveMatch_list <- match(stemmed_review, poslist)
negativeMatch_list <- match(stemmed_review, neglist)
#calculate sum of number of words in each matched category
positiveMatch_list <- sum(!is.na(positiveMatch_list))
negativeMatch_list <- sum(!is.na(negativeMatch_list))
#total <- positiveMatch_list + negativeMatch_list
#posRatio <- (positiveMatch_list/total)
#negRatio <- (negativeMatch_list/total)
score <- c(positiveMatch_list, negativeMatch_list)
#add row to scores table
newrow <- c(initial_review, score)
result  <- rbind(result, newrow)
return(result)
}, neglist, poslist)
return(reviews_scores)
}
fin_Result <- as.data.frame(sentimentAnalysis(myCorpus, neglist, poslist))
fin_Result <- as.data.frame(sentimentAnalysis(myCorpus, neglist, poslist))
fin_Result <- as.data.frame(sentimentAnalysis(myCorpus, neglist, poslist))
fin_Result <- as.data.frame(sentimentAnalysis(myCorpus, neglist, poslist))
library(deepnet)
setwd("E:/ADS/Final Project")
final<-read.csv( "final_5000.csv", header= TRUE)
train<-final[2:401,]
test<-final[402:501,]
train.x<-as.matrix(train[, 2:3180])
train.y<- as.matrix(train$RESULT)
test.x<-as.matrix(test[, 2:3180])
test.y<-as.matrix(test$RESULT)
dnn <- dbn.dnn.train(train.x, train.y, hidden = c(500,500, 500), numepochs = 5, cd=2)
err.dnn <- nn.test(dnn, test.x, test.y)
err.dnn
yy.dnn <- nn.predict(dnn, test.x)
yy.dnn
err.dnn <- nn.test(dnn, test.x, test.y)
err.dnn
yy.dnn <- nn.predict(dnn, test.x)
yy.dnn
table(yy.dnn, test.y)
test.x<-as.matrix(test[, 2:3180])
test.y<-as.matrix(test$RESULT)
err.dnn <- nn.test(dnn, test.x, test.y)
err.dnn <- nn.test(dnn, test.x, test.y)
err.dnn
yy.dnn <- nn.predict(dnn, test.x)
yy.dnn
predictions <- unlist(yy.dnn)
dbn.out<-as.data.frame(predictions)
nrow(dbn.out)
xtab <- table(unlist(test.x),unlist(round(yy.dnn)))
xtab
xtab <- table(unlist(test.x),unlist(round(yy.dnn)))
xtab
xtab <- table(unlist(test.x),unlist(round(yy.dnn)))
xtab <- table(unlist(test.x),unlist(round(yy.dnn)))
xtab <- table(unlist(test.y),unlist(round(yy.dnn)))
xtab
a <- confusionMatrix(unlist(round(yy.dnn)), unlist(test.y))
library(caret)
a <- confusionMatrix(unlist(round(yy.dnn)), unlist(test.y))
a
library(deepnet)
setwd("E:/ADS/Final Project")
final<-read.csv( "final_temp.csv", header= TRUE)
train<-final[2:401,]
test<-final[402:501,]
train.x<-as.matrix(train[, 2:3180])
train.y<- as.matrix(train$RESULT)
test.x<-as.matrix(test[, 2:3180])
library(deepnet)
setwd("E:/ADS/Final Project")
final<-read.csv( "final_temp1.csv", header= TRUE)
train<-final[2:401,]
test<-final[402:501,]
train.x<-as.matrix(train[, 2:3180])
train.y<- as.matrix(train$RESULT)
test.x<-as.matrix(test[, 2:3180])
test.y<-as.matrix(test$RESULT)
dnn <- dbn.dnn.train(train.x, train.y, hidden = c(500,500, 500), numepochs = 5, cd=2)
err.dnn <- nn.test(dnn, test.x, test.y)
err.dnn
yy.dnn <- nn.predict(dnn, test.x)
yy.dnn
predictions <- unlist(yy.dnn)
dbn.out<-as.data.frame(predictions)
nrow(dbn.out)
xtab <- table(unlist(test.y),unlist(round(yy.dnn)))
xtab
library(deepnet)
setwd("E:/ADS/Final Project")
final<-read.csv( "final_temp1.csv", header= TRUE)
train<-final[2:401,]
train.x<-as.matrix(train[, 2:6249])
train.y<- as.matrix(train$RESULT)
test.x<-as.matrix(test[, 2:6249])
test.y<-as.matrix(test$RESULT)
library(deepnet)
setwd("E:/ADS/Final Project")
train<-final[2:4001,]
train<-final[2:4001,]
library(deepnet)
setwd("E:/ADS/Final Project")
final<-read.csv( "final_temp1.csv", header= TRUE)
train<-final[2:4001,]
test<-final[4002:5001,]
train.x<-as.matrix(train[, 2:6249])
train.y<- as.matrix(train$RESULT)
test.x<-as.matrix(test[, 2:6249])
test.y<-as.matrix(test$RESULT)
dnn <- dbn.dnn.train(train.x, train.y, hidden = c(500,500,500,500), numepochs = 5, cd=2)
yy.dnn <- nn.predict(dnn, test.x)
yy.dnn
predictions <- unlist(yy.dnn)
dbn.out<-as.data.frame(predictions)
nrow(dbn.out)
xtab <- table(unlist(test.y),unlist(round(yy.dnn)))
xtab
err.dnn <- nn.test(dnn, test.x, test.y)
err.dnn
err.dnn
err.dnn <- nn.test(dnn, test.x, test.y)
err.dnn
library(caret)
a <- confusionMatrix(unlist(round(yy.dnn)), unlist(test.y))
a
view(final)
View(final)
NCOL(train)
nCOL(train)
library(deepnet)
setwd("E:/ADS/Final Project")
final<-read.csv( "final_temp1.csv", header= TRUE)
View(final)
train<-final[1:4000,]
test<-final[4001:5000,]
nCOL(train)
ncol(train)
train.x<-as.matrix(train[, 2:6250])
train.y<- as.matrix(train$RESULT)
test.x<-as.matrix(test[, 2:6250])
test.y<-as.matrix(test$RESULT)
dnn <- dbn.dnn.train(train.x, train.y, hidden = c(500,500,500,500), numepochs = 5, cd=2)
yy.dnn <- nn.predict(dnn, test.x)
yy.dnn
predictions <- unlist(yy.dnn)
dbn.out<-as.data.frame(predictions)
nrow(dbn.out)
xtab <- table(unlist(test.y),unlist(round(yy.dnn)))
xtab
err.dnn <- nn.test(dnn, test.x, test.y)
err.dnn
library(caret)
a <- confusionMatrix(unlist(round(yy.dnn)), unlist(test.y))
yy.dnn<-yy.dnn[2]
yy.dnn
yy.dnn <- nn.predict(dnn, test.x)
yy.dnn<-yy.dnn[,2]
ncol(train)
library(deepnet)
setwd("E:/ADS/Final Project")
final<-read.csv( "final_temp1.csv", header= TRUE)
train<-final[1:4000,]
test<-final[4001:5000,]
train.x<-as.matrix(train[, 1:6250])
train.y<- as.matrix(train$RESULT)
test.x<-as.matrix(test[, 1:6250])
test.y<-as.matrix(test$RESULT)
dnn <- dbn.dnn.train(train.x, train.y, hidden = c(500,500,500,500), numepochs = 5, cd=2)
yy.dnn <- nn.predict(dnn, test.x)
yy.dnn<-yy.dnn[,2]
yy.dnn<-yy.dnn[,2]
yy.dnn<-yy.dnn[,1]
yy.dnn
predictions <- unlist(yy.dnn)
dbn.out<-as.data.frame(predictions)
nrow(dbn.out)
xtab <- table(unlist(test.y),unlist(round(yy.dnn)))
xtab
err.dnn <- nn.test(dnn, test.x, test.y)
err.dnn
library(caret)
a <- confusionMatrix(unlist(round(yy.dnn)), unlist(test.y))
a
train<-final[2:4001,]
final<-read.csv( "final_temp1.csv", header= TRUE)
train<-final[2:4001,]
test<-final[4002:5001,]
train.x<-as.matrix(train[, 2:6250])
train.y<- as.matrix(train$RESULT)
test.x<-as.matrix(test[, 2:6250])
test.y<-as.matrix(test$RESULT)
dnn <- dbn.dnn.train(train.x, train.y, hidden = c(500,500,500,500), numepochs = 5, cd=2)
yy.dnn <- nn.predict(dnn, test.x)
yy.dnn<-yy.dnn[,1]
yy.dnn
View(yy.dnn)
predictions <- unlist(yy.dnn)
predictions
dbn.out<-as.data.frame(predictions)
nrow(dbn.out)
View(
dbn.out)
xtab <- table(unlist(test.y),unlist(round(dbn.out)))
xtab
err.dnn <- nn.test(dnn, test.x, test.y)
err.dnn
library(caret)
a <- confusionMatrix(unlist(round(yy.dnn)), unlist(test.y))
a <- confusionMatrix(unlist(round(dbn.out)), unlist(test.y))
a
a <- confusionMatrix(unlist(round(dbn.out)), unlist(test.y))
View(test.y)
dbn.out<-as.matrix(dbn.out)
xtab <- table(unlist(test.y),unlist(round(dbn.out)))
xtab
err.dnn <- nn.test(dnn, test.x, test.y)
err.dnn
library(caret)
a <- confusionMatrix(unlist(round(dbn.out)), unlist(test.y))
dbn.out<-as.matrix(dbn.out)
View(dbn.out)
setwd("E:/ADS/Final Project")
final<-read.csv( "final_temp1.csv", header= TRUE)
final<-read.csv( "final_temp1.csv", header= TRUE)
train<-final[1:4000,]
test<-final[4001:5000,]
train.x<-as.matrix(train[, 2:6250])
train.y<- as.matrix(train$RESULT)
test.x<-as.matrix(test[, 2:6250])
test.y<-as.matrix(test$RESULT)
dnn <- dbn.dnn.train(train.x, train.y, hidden = c(500,500,500,500), numepochs = 5, cd=2)
dnn <- dbn.dnn.train(train.x, train.y, hidden = c(500,500,500), numepochs = 5, cd=2)
yy.dnn <- nn.predict(dnn, test.x)
yy.dnn<-yy.dnn[,1]
View(yy.dnn)
predictions <- unlist(yy.dnn)
predictions
dbn.out<-as.data.frame(predictions)
nrow(dbn.out)
View(dbn.out)
xtab <- table(unlist(test.y),unlist(round(dbn.out)))
xtab
err.dnn <- nn.test(dnn, test.x, test.y)
err.dnn
library(caret)
a <- confusionMatrix(unlist(round(dbn.out)), unlist(test.y))
a
a <- confusionMatrix(round(dbn.out), unlist(test.y))
a <- confusionMatrix(unlist(round(dbn.out)), unlist(test.y))
a <- confusionMatrix(round(dbn.out), test.y)
a <- confusionMatrix(unlist((dbn.out)), unlist(test.y))
dbn.out<-as.matrix(dbn.out)
View(dbn.out)
a <- confusionMatrix(unlist((dbn.out)), unlist(test.y))
a <- confusionMatrix((dbn.out), unlist(test.y))
View(dbn.out)
a <- confusionMatrix(unlist((dbn.out), unlist(test.y))
a
a <- confusionMatrix(unlist(dbn.out), unlist(test.y))
??confusionMatrix
a <- confusionMatrix(as.factor(dbn.out), unlist(test.y))
a <- confusionMatrix(as.factor(dbn.out), as.factor(unlist(test.y)
a
a <- confusionMatrix(as.factor(dbn.out), as.factor(unlist(test.y))
a
a <- confusionMatrix(as.factor(dbn.out), as.factor(test.y))
View(test.y)
a <- confusionMatrix(as.factor(dbn.out), as.dataframe(test.y))
a <- confusionMatrix(as.factor(dbn.out), as.data.frame(test.y))
b<-confusionMatrix(xtab)
yy.dnn <- nn.predict(dnn, test.x)
yy.dnn<-yy.dnn[,1]
yy.dnn <- nn.predict(dnn, test.x)
yy.dnn
predictions <- unlist(yy.dnn)
predictions
a <- confusionMatrix(as.factor(yy.dnn), as.data.frame(test.y))
a <- confusionMatrix(unlist(yy.dnn), unlist(test.y))
a <- confusionMatrix(unlist(yy.dnn), unlist(test.y))
a
err.dnn <- nn.test(dnn, test.x, test.y)
err.dnn
summary(yy.dnn)
